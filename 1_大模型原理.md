# 1. 引言

## 1.1 定义

具有大规模参数和复杂计算结构的深度学习模型。

## 1.2 涌现能力

​	在2022年的一篇论文提出，在较小的模型中不出现，而在较大的模型中出现的能力，则可以称之为**emergent**。当模型的训练数据和参数不断扩大，直到达到一定的临界规模后，其表现出了一些**未能预测的、更复杂的能力和特性**，模型能够从原始训练数据中自动学习并**发现新的、更高层次的特征和模式**。**区别大模型和传统小模型的核心点。**

<img src="pictures\image-20241123092018001.png" alt="image-20241123092018001" style="zoom:67%;" />

## 1.3 发展背景

<img src="pictures\image-20241124145449016.png" alt="image-20241124145449016" style="zoom:67%;" />

萌芽期（1950-2005）：以 CNN 为代表的传统神经网络模型阶段

探索沉淀期（2006-2019）：以 Transformer 为代表的全新神经网络模型阶段

迅猛发展期（2020-至今）：以 `GPT` 为代表的预训练大模型阶段

## 1.4 未来发展

### 1.4.1 幻觉问题

​	大模型的幻觉问题指的是在生成文本或其他类型输出时，**模型产生的信息与事实不符或完全虚构的现象**，严重影响了模型的可靠性和可用性。幻觉问题分为两个方面： 

- 事实性幻觉（Factuality Hallucination）：模型生成的内容包含了事实上不正确或完全虚构的信息。
- 忠实性幻觉（Faithfulness Hallucination）：模型生成的内容虽然可能在某种程度上看起来合理，但并不忠实于输入信息或背景材料。 

> 数据去骗与过滤：低质量训练数据带来的信息错误与知识缺失

> 知识检索增强：检索对齐世界知识修正或减轻幻觉

> 解码过程增强：优化解码提高上下文的忠实性

### 1.4.2 计算与资源瓶颈

量化：降低权重和激活值的计算精度来减少模型的内存占用

剪枝：移除给定模型中的部分权重，而不降低其性能

蒸馏：将大模型的知识蒸馏到更小的模型

# 2. 深度神经网络基础

## 2.1 神经元与神经网络

<img src="pictures\image-20241124150011428.png" alt="image-20241124150011428" style="zoom:67%;" />
$$
输入层：神经元接收来自上一层（输入层或隐藏层）的信号，每个输入都有一个权重。\\
加权求和：神经元将输入信号与对应的权重进行加权求和，并加上一个偏置项 b，得到一个总和 z=∑_n^{i=1}w_ix_i+b 。\\
激活函数：加权求和的结果通过一个激活函数进行非线性变换，得到神经元的输出。这个输出会被传递给下一层神经元。\\
$$
<img src="pictures\image-20241124150247453.png" alt="image-20241124150247453" style="zoom:67%;" />

- 输入层：接收原始输入数据。

- 隐藏层：由多个神经元组成，进行特征抽取和数据处理，通常包含多个隐藏层（深度神经网络就是多层隐藏层的网络，关键是深度性和激活函数的非线性）。

- 输出层：输出网络的最终结果，通常用于回归问题（例如预测值）或分类问题（例如标签分类）。

## 2.2 激活函数

![image-20241124150414746](pictures\image-20241124150414746.png)

- Sigmoid 函数
  Sigmoid 是一个S型的函数，输出值范围在 (0, 1) 之间，常用在二分类任务中，表示概率。
  特点：由于输出范围限制在 (0, 1)，它可以将任意输入映射到0到1之间，常用于输出层（尤其是二分类问题）。
  由于**梯度消失问题**，Sigmoid 在训练深层网络时效果不佳，尤其是在层数较多时，**梯度在反向传播时可能变得非常小**，导致学习速度变慢。

- `Tanh` 函数
  `Tanh` 函数与 Sigmoid 函数类似，但它的输出范围是 (-1, 1)。
  特点：与 Sigmoid 类似，但输出范围为 (-1, 1)，这使得它的梯度在训练过程中更加稳定。
  然而，`Tanh` 也存在梯度消失的问题，特别是在较深的网络中，仍然会导致训练效率低下。

- `ReLU`（Rectified Linear Unit）整流线性单元
  `ReLU` 是目前最常用的激活函数之一。
  特点：非线性，且非常简单，计算速度快。输出范围是 (0, ∞)，对正输入值有良好的响应，对于负值输出0，避免了梯度消失问题。`ReLU` 的缺点是它可能会**“死亡”**——即在反向传播过程中，**某些神经元的梯度始终为0，导致这些神经元的参数永远无法更新**。为了解决这一问题，衍生出了 Leaky `ReLU` 和 Parametric `ReLU`。
- Leaky `ReLU`
  Leaky `ReLU` 是对 `ReLU` 的一种改进，它允许在输入为负时，输出一个小的线性值（而非完全为0）。其中α是一个很小的常数特点：通过引入一个小的斜率（通常是0.01），避免了`ReLU`**“死神经元”问题**。

- `Softmax` 函数
  `Softmax` 常用于多分类任务的输出层，将一个向量转换为概率分布。对于每个类，它计算其相对概率，公式为：
  $$
  Softmax(x_i)=e^x_i∕∑_n^{j=1}e^x_j
  $$
  特点：输出是一个概率分布，所有输出值的和为1，适用于分类任务。

## 2.3 正则化

正则化技术用于**减少模型的复杂度，防止过拟合（`overfitting`）**。过拟合是指模型在训练数据上表现良好，但在测试数据上性能较差，通常是由于模型在训练过程中“记住”了噪声而不是学习到数据的实际规律。以下是几种常见的正则化技术（`L1`正则化有助于稀疏化权重，`L2`正则化则会使权重趋向较小值。）

`L2`正则化：惩罚参数的平方和
$$
L2= λ\sum_n^{i=1}w_i^2
$$
`L2` 正则化可以有效地减少模型的复杂度，使得参数保持在较小的范围内。`L2` 正则化的效果是使得一些**权重变得非常小，接近于零，但通常不会将其完全归零。**

`L1`正则化：惩罚参数的绝对值之和
$$
L1=λ\sum_n^{i=1}|w_i|
$$
`L1` 正则化的效果是**使得一些权重完全为零**，这也有助于特征选择，自动去除不重要的特征。在高维稀疏数据中，`L1` 正则化常常能够获得更简洁的模型。

Dropout：随机丢弃神经元

在每个训练步骤中，按一定的概率随机“丢弃”一些神经元，使得网络不依赖于任何单一的神经元，防止某些神经元对结果过于敏感。**Dropout通常在训练时使用，而在测试时会恢复所有神经元的计算。**

## 2.4 优化算法 

目的是最小化损失函数

### 2.4.1 梯度下降

计算梯度 → 更新参数、调整学习率

计算梯度：通过反向传播算法计算损失函数对每个参数的梯度。

更新参数：根据梯度信息更新模型的参数，通常通过调整学习率来控制更新的步伐。

**例子：**

假设损失函数为如下函数：
$$
L(θ)=(θ−3)^2
$$

```python3
import numpy as np
from tqdm import tqdm

def loss_function(theta):
    return (theta - 3) ** 2

def gradient(theta):
    return 2 * (theta - 3)

# 初始化参数
theta = 0
learning_rate = 0.1
iterations = 100
update_step = 10  # 每 10 次更新一次

# 手动控制 tqdm 的更新
with tqdm(total=iterations, desc="Training Progress", ncols=100) as pbar:
    for i in range(iterations):
        grad = gradient(theta)
        theta -= learning_rate * grad
        
        loss = loss_function(theta)
        
        # 每 10 次打印状态和更新进度条
        if (i + 1) % update_step == 0:  # 修改为 (i + 1)，确保 0 也打印
            print(f"DEBUG: i = {i}, update_step = {update_step}")
            tqdm.write(f"Iteration {i + 1}: theta = {theta:.4f}, loss = {loss:.4f}")
            pbar.update(update_step)  # 手动更新进度条

# 最终结果
print(f"Optimal theta: {theta:.4f}")

```

<img src="pictures\image-20241125105757268.png" alt="image-20241125105757268" style="zoom:80%;" />

> 然后注意使用的时候需要调用`tqdm`模块中的`tqdm`函数才行

> 这个`tqdm`存在吞输出的问题，具体的`tqdm`的工作原理还有待探究。

### 2.4.2 随机梯度下降(`SGD`)

### 2.4.3 `Adam`

### 2.4.4 `LAMB`

## 2.5 自适应学习率

## 2.6 动态调整

# 3. 语言模型

## 3.1 基于统计方法的语言模型

n阶马尔可夫链

## 3.2 基于循环神经网络的语言模型

RNN

LSTM

GRU

## 3.3 基于transformer架构的语言模型

位置编码

多头注意力机制

encoder 

decoder

## 3.4 采样方法

贪婪采样

随机采样

温度采样

束搜索

## 3.5 评测方法

困惑度

BLEU

ROUGE

人工评测

# 4. 大模型架构

## 4.1 注意力机制

## 4.2 transformer架构

纯Encoder

Encoder-Decoder

纯Decoder

## 4.3 多任务学习



# 5. 自监督学习和预训练技术

